{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ab735f4",
   "metadata": {},
   "source": [
    "Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9d6785",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Sastrawi --quiet\n",
    "!pip install tensorflow --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855937c3",
   "metadata": {},
   "source": [
    "Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faef0d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import re, io, json\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint   # save model\n",
    "from tensorflow.keras.models import load_model   # load saved model\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "\n",
    "# Indonesian Stemmer\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913e5256",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30affba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./utf8_dataset.csv')\n",
    "data.dropna(subset=['Tweet'], how='all', inplace=True)\n",
    "data = data[['Tweet','HS']]\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab48935",
   "metadata": {},
   "source": [
    "Check dataset details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5801e504",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['HS'].size, \"Total\")\n",
    "print(np.sum(data['HS'] == 1), \"Hate speech\")\n",
    "print(np.sum(data['HS'] == 0), \"Non hate speech\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b712979e",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6797f442",
   "metadata": {},
   "source": [
    "### Make everything lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae18735",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Tweet'] = data['Tweet'].apply(lambda tweet: tweet.lower())\n",
    "data['Tweet'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef6f3b5",
   "metadata": {},
   "source": [
    "### Remove known unwanted words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e2e06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \\n \\t \\r\n",
    "data['Tweet'].replace(to_replace=[r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"], value=[\" \",\" \"], regex=True, inplace=True)\n",
    "\n",
    "# Remove RT\n",
    "data['Tweet'] = data['Tweet'].str.replace('rt', '')\n",
    "\n",
    "# Remove USER\n",
    "data['Tweet'] = data['Tweet'].str.replace('user', '')\n",
    "\n",
    "# Remove URL\n",
    "data['Tweet'] = data['Tweet'].str.replace('url', '')\n",
    "\n",
    "data['Tweet'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b58a872",
   "metadata": {},
   "source": [
    "### Remove non-alphabets characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0164bfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Tweet'] = data['Tweet'].replace({'[^A-Za-z]': ' '}, regex = True)\n",
    "data['Tweet'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0fb0e7",
   "metadata": {},
   "source": [
    "### Remove words that is less than 3 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163e58bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Tweet'] = data['Tweet'].apply(lambda tweet: ' '.join([w for w in tweet.split() if len(w) > 2]))\n",
    "print(data['Tweet'].head(10));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bf0fc4",
   "metadata": {},
   "source": [
    "### Reformat texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab714e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove excess spaces\n",
    "data['Tweet'] = data['Tweet'].apply(lambda tweet: ' '.join(tweet.split()))\n",
    "\n",
    "# Trim\n",
    "data['Tweet'] = data['Tweet'].str.strip()\n",
    "\n",
    "data['Tweet'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6569dd46",
   "metadata": {},
   "source": [
    "### Load and replace alay words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59e29fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "alay_words = pd.read_csv('alay.csv')\n",
    "alay_words.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069429f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_alay(tweet):\n",
    "    output = []\n",
    "    words = tweet.split()\n",
    "    for word in words:\n",
    "      row = alay_words[alay_words.alay == word]\n",
    "      if row.empty:\n",
    "        output.append(word)\n",
    "      else:\n",
    "        output.append(str(row['replacement'].values[0]))\n",
    "\n",
    "    return ' '.join(output)\n",
    "\n",
    "data['Tweet'] = data['Tweet'].apply(lambda tweet: replace_alay(tweet))\n",
    "data['Tweet'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600cb7f5",
   "metadata": {},
   "source": [
    "### Load and remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56df2671",
   "metadata": {},
   "outputs": [],
   "source": [
    "indonesian_stopwords = pd.read_csv('stopwords.txt', sep=\"\\n\")\n",
    "indonesian_stopwords = indonesian_stopwords.iloc[:, 0].values.tolist()\n",
    "indonesian_stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b09821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tweet):\n",
    "    output = []\n",
    "    words = tweet.split()\n",
    "    for word in words:\n",
    "      if word not in indonesian_stopwords:\n",
    "        output.append(word)\n",
    "\n",
    "    return ' '.join(output)\n",
    "\n",
    "data['Tweet'] = data['Tweet'].apply(lambda tweet: remove_stopwords(tweet))\n",
    "\n",
    "data['Tweet'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bc9be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Tweet'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9caf38d",
   "metadata": {},
   "source": [
    "### Stem using Indonesian stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419362d8",
   "metadata": {},
   "source": [
    "It took quite some time, measured to be around 1 hour and 40 minutes, so be patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bff5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer_factory = StemmerFactory()\n",
    "stemmer = stemmer_factory.create_stemmer()\n",
    "\n",
    "data['Tweet'] = data['Tweet'].apply(lambda tweet: stemmer.stem(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4936af4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Tweet'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc92113",
   "metadata": {},
   "source": [
    "### Tokenize the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d0505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna()\n",
    "\n",
    "max_features = 2000\n",
    "tokenizer = Tokenizer(lower=False, num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(data['Tweet'].values)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(data['Tweet'].values)\n",
    "X = pad_sequences(X)\n",
    "\n",
    "X[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4742450f",
   "metadata": {},
   "source": [
    "## Training\n",
    "### Initialize LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0141e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_dim,input_length = X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "print(X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19abf457",
   "metadata": {},
   "source": [
    "### Split dataset for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f110f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.get_dummies(data['HS']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.20, random_state = 42)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d793971",
   "metadata": {},
   "source": [
    "### Declare checkpoint to save the model as a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb73c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/LSTM_twitter_sentiment_analysis_latest.h5'\n",
    "checkpoint = ModelCheckpoint(\n",
    "    model_path,\n",
    "    monitor='accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12df2a6e",
   "metadata": {},
   "source": [
    "### Start training with 15 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2315370",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "model.fit(X_train, Y_train, epochs = 15, batch_size=batch_size, verbose = 1, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e00602",
   "metadata": {},
   "source": [
    "### Measure score and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95b631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_x = model.predict(X_test)\n",
    "classes_x = np.argmax(predict_x, axis=1)\n",
    "\n",
    "df_test = pd.DataFrame({'true': Y_test.tolist(), 'pred': classes_x})\n",
    "print(df_test.head())\n",
    "\n",
    "df_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\n",
    "\n",
    "print('confusion matrix', confusion_matrix(df_test.true, df_test.pred))\n",
    "print(classification_report(df_test.true, df_test.pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b8d48b",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9b46cd",
   "metadata": {},
   "source": [
    "### Load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ca33e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636a329c",
   "metadata": {},
   "source": [
    "### Accept input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f13cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"itu cebong ngapain demo di monas, mending tiduran dirumah\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f7c540",
   "metadata": {},
   "source": [
    "### Run preprocessing on the input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95497671",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = replace_alay(tweet)\n",
    "tweet = remove_stopwords(tweet)\n",
    "tweet = stemmer.stem(tweet)\n",
    "\n",
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8484a81f",
   "metadata": {},
   "source": [
    "### Tokenize inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17384ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_word = tokenizer.texts_to_sequences([tweet])\n",
    "tokenized_word = pad_sequences(tokenized_word, maxlen=38, dtype='int32', value=0)\n",
    "\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e76c3ba",
   "metadata": {},
   "source": [
    "### Run prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69717cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = loaded_model.predict(tokenized_word,batch_size=1)[0]\n",
    "\n",
    "if(np.argmax(sentiment) == 0):\n",
    "    print(\"Not a hate speech,\", sentiment[0], 'sure')\n",
    "elif (np.argmax(sentiment) == 1):\n",
    "    print(\"Hate speech,\", sentiment[1], 'sure')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
